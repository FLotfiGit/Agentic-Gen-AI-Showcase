{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a022013a",
   "metadata": {},
   "source": [
    "# CLIP + LLM Cooperative Reasoning\n",
    "This notebook demonstrates multimodal reasoning by combining CLIP (vision-language) and an LLM (text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53fe918",
   "metadata": {},
   "source": [
    "# CLIP + LLM Cooperative Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d24792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install openai torch torchvision pillow git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, clip, openai, json, requests\n",
    "from PIL import Image\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', None)\n",
    "assert openai.api_key, 'Set OPENAI_API_KEY in your environment!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# Load image and encode\n",
    "img_url = 'https://images.unsplash.com/photo-1506744038136-46273834b3fb'\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    img_features = model.encode_image(image_input).cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM reasoning over image embedding\n",
    "prompt = f\"Image embedding: {img_features[:8]}...\\nDescribe what this image might contain and its context.\"\n",
    "resp = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=[{'role':'user','content':prompt}])\n",
    "desc = resp.choices[0].message.content\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "with open('./outputs/clip_llm_output.json', 'w') as f:\n",
    "    json.dump({'desc': desc}, f, indent=2)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6349223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs\n",
    "import json, os\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "image.save('./outputs/multimodal_input.jpg')\n",
    "with open('./outputs/clip_embedding.json', 'w') as f:\n",
    "    json.dump({'embedding': img_features[0][:128]}, f, indent=2)\n",
    "print('Saved image and embedding to outputs/.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f682983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image and embedding with timestamp\n",
    "import os, json, time\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "ts = int(time.time())\n",
    "img_path = f'./outputs/multimodal_input_{ts}.jpg'\n",
    "image.save(img_path)\n",
    "emb_path = f'./outputs/clip_embedding_{ts}.json'\n",
    "with open(emb_path, 'w') as f:\n",
    "    json.dump({'embedding_dim': len(img_features[0]), 'embedding_head': img_features[0][:16]}, f, indent=2)\n",
    "print('Saved', img_path, 'and', emb_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
